# hyh佬分享的大量数据面试题的相关总结

## 5亿个数中找出中位数

### 无内存限制情况

维护两个堆, 一个大顶堆, 一个小顶堆

见: [力扣295题](https://leetcode-cn.com/problems/find-median-from-data-stream/)

[std::priority_queue](https://www.cplusplus.com/reference/queue/priority_queue/)

```c++
#include<deque>
class MedianFinder {
    priority_queue<int> lo;                              // max heap
    priority_queue<int, vector<int>, greater<int>> hi;   // min heap

public:
    // Adds a number into the data structure.
    void addNum(int num)
    {
        lo.push(num);                                    // Add to max heap

        hi.push(lo.top());                               // balancing step
        lo.pop();

        if (lo.size() < hi.size()) {                     // maintain size property
            lo.push(hi.top());
            hi.pop();
        }
    }

    // Returns the median of current data stream
    double findMedian()
    {
        return lo.size() > hi.size() ? (double) lo.top() : (lo.top() + hi.top()) * 0.5;
    }
};

```

###  有内存限制情况

5亿个数, 一个数占4B, 总共需要2G内存, 若可用内存小于2G, 则需要使用分治法

分治法是将一个大的问题逐渐转换为规模较小的问题来求解, 对应这道题, 通过二进制位将数据分类, 先从最高位开始, 通过最高位是1还是0, 将数据分为两类, 中位数存在数据量较大的一类(若一样大则为0类中的最大值和1类中的最小值平均数), 如此一直递归划分, 知道数据量可以加载到内存中

# 多个数组中的TopK数

## 大批量数据按出现频度排序

有十个文件, 每个文件大小为1G, 每个文件的每一行都是用户的query, 每个文件的query都可能重复, 按照query出现的频度进行排序

###  HashMap法

若query的重复率比较高, 则不同的query的数量应较小, 可以将所有的query都加载到Map中

###  分治法

若query的重复率较低, 可以遍历这10个文件的query, 根据hash(query) % 10 将 query 重新划分到10个文件中, 此时文件中重复率较高, 不同query的数量较小, 将所有query读入Map, 根据频度排序写入另外一个文件, 使用归并排序将所有query排序(外排序, 败者树)

# 两个存有大量数据的文件找出相同数据

给定a, b两个文件, 各存放50亿个URL, 每个URL占64B, 内存限制是4G, 找出a, b两个文件共同的URL

50亿个URL占用空间大小约为320GB, 按照Hash(URL) % n, 将a, b分为ai, bi多个小文件, 遍历ai, 将URL存储到Set中, 遍历bi中的URL, 若ai中存在, 则为共同的URL, 存入到单独的文件中

## 迅速判断整形数是否出现过

### 位图法:

给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中

申请512M的内存

一个bit位代表一个unsigned int值

读入40亿个数，设置相应的bit位

读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在

## 大量数据中找出频度TopK

1GB的文件, 文件每一行是一个词, 每个词的大小不超过16B, 内存限制是1MB, 返回频度最高的100个词

遍历文件中每个词x, 执行hash(x) % 5000, 存入对应的小文件, 每次读入一个小文件, 使用hashMap得到小文件中的top100, 维护一个小顶堆, 得到所有词的top100

## 大量字符串的频度topK问题

搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。

假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

#### 方法一：分治法

分治法依然是一个非常实用的方法。

划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。

方法可行，但不是最好，下面介绍其他方法。

#### 方法二：HashMap 法

虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。

思路如下：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 `O(N)`。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 `O(Nlog10)`。

#### 方法三：前缀树法

方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

思路如下：

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。

### 方法总结

**前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。**
